# Model configuration registry
# Maps model names to provider settings and parameters
# 
# Default values (from .env):
#   temperature: 0
#   max_tokens: 2048

# Anthropic Models
claude-3-5-sonnet:
  provider: anthropic
  model_id: claude-3-5-sonnet-20241022
  max_tokens: 4096  # Override default
  description: "Claude 3.5 Sonnet - non-reasoning model"

claude-3-5-haiku:
  provider: anthropic  
  model_id: claude-3-5-haiku-20241022
  max_tokens: 4096  # Override default
  description: "Claude 3.5 Haiku - faster, non-reasoning model"

# OpenAI Models  
gpt-4o:
  provider: openai
  model_id: gpt-4o
  max_tokens: 4096  # Override default
  description: "GPT-4 Omni - latest OpenAI model"

gpt-4o-mini:
  provider: openai
  model_id: gpt-4o-mini  
  max_tokens: 4096  # Override default
  description: "GPT-4 Omni Mini - cost-effective version"

# Google Models
gemini-pro:
  provider: google
  model_id: gemini-1.5-pro
  max_tokens: 4096  # Override default
  description: "Gemini 1.5 Pro - Google's flagship model"

gemini-flash:
  provider: google
  model_id: gemini-1.5-flash
  max_tokens: 8192  # Override default
  description: "Gemini 1.5 Flash - faster inference"

# Verification model (used for answer extraction)
gemini-2-5-flash:
  provider: google
  model_id: gemini-2.0-flash-exp
  max_tokens: 1024  # Override default - smaller for verification
  description: "Gemini 2.0 Flash - for answer extraction and CoT verification"

# Gemma Models
gemma-3-4b-it:
  provider: google
  model_id: gemma-3-4b-it
  # Uses default temperature: 0
  # Uses default max_tokens: 2048
  description: "Gemma 3 4B Instruct - Google's open model"

# Featherless AI Models (via OpenAI-compatible API)
llama-3.1-8b:
  provider: featherless
  model_id: meta-llama/Meta-Llama-3.1-8B-Instruct
  max_tokens: 4096  # Override default
  description: "Llama 3.1 8B via Featherless AI"

llama-3.1-70b:
  provider: featherless
  model_id: meta-llama/Meta-Llama-3.1-70B-Instruct
  max_tokens: 4096  # Override default
  description: "Llama 3.1 70B via Featherless AI"

deepseek-v2.5:
  provider: featherless
  model_id: deepseek-ai/DeepSeek-V2.5
  max_tokens: 4096  # Override default
  description: "DeepSeek V2.5 via Featherless AI"

qwen-2.5-72b:
  provider: featherless
  model_id: Qwen/Qwen2.5-72B-Instruct
  max_tokens: 4096  # Override default
  description: "Qwen 2.5 72B via Featherless AI"

# Groq Models (fast inference)
llama-3.1-8b-groq:
  provider: groq
  model_id: llama-3.1-8b-instant
  max_tokens: 4096  # Override default
  description: "Llama 3.1 8B via Groq (fast)"

llama-3.1-70b-groq:
  provider: groq
  model_id: llama-3.1-70b-versatile
  max_tokens: 4096  # Override default
  description: "Llama 3.1 70B via Groq (fast)"

llama-3.3-70b-groq:
  provider: groq
  model_id: llama-3.3-70b-versatile
  max_tokens: 4096  # Override default
  description: "Llama 3.3 70B via Groq (fast)"

mixtral-8x7b-groq:
  provider: groq
  model_id: mixtral-8x7b-32768
  max_tokens: 4096  # Override default
  description: "Mixtral 8x7B via Groq (fast)"

# Azure AI Models (serverless API endpoints)
llama-3.1-8b-azure:
  provider: azure
  model_id: Meta-Llama-3.1-8B-Instruct
  # Uses default temperature: 0
  # Uses default max_tokens: 2048
  description: "Llama 3.1 8B via Azure AI"

llama-3.1-70b-azure:
  provider: azure
  model_id: Meta-Llama-3.1-70B-Instruct
  # Uses default temperature: 0
  # Uses default max_tokens: 2048
  description: "Llama 3.1 70B via Azure AI"

# Add more Azure models as needed
# Note: model_id should match the exact model name in your Azure deployment